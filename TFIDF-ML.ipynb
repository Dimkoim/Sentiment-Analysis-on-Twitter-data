{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "def arrangeTweet(tweet):\n",
    "    if ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet)) in english_vocab:\n",
    "        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    else:\n",
    "        return ''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "oldfile=open(\"train_pos.txt\",'rb')\n",
    "sentences=[]\n",
    "i=0\n",
    "for line in oldfile:\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    \n",
    "    line = line.decode('utf8')\n",
    "    line=arrangeTweet(line)\n",
    "    #if not (line in sentences):\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "oldfile=open(\"train_neg.txt\",'rb')\n",
    "i=0\n",
    "for line in oldfile:\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    \n",
    "    line = line.decode('utf8')\n",
    "    line=arrangeTweet(line)\n",
    "    #if not (line in sentences):\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes=np.arange(0,200000)\n",
    "np.random.shuffle(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=[0]*100000\n",
    "y2=[1]*100000\n",
    "y=y+y2\n",
    "y=np.array(y)\n",
    "y=y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scikit_TFIDF(m,n,Total_clean_train,Total_clean_test):\n",
    "    vectorizer = CountVectorizer(min_df=1,max_features=m,ngram_range=(1,n))\n",
    "    analyze = vectorizer.build_analyzer\n",
    "    X_train = vectorizer.fit_transform(Total_clean_train).toarray()\n",
    "    X_test = vectorizer.transform(Total_clean_test).toarray()\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf_train=transformer.fit_transform(X_train).toarray()\n",
    "    tfidf_test=transformer.transform(X_test).toarray()\n",
    "    return tfidf_train,tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "testfile=open(\"test_data.txt\",'rb')\n",
    "i=0\n",
    "sentences_test=[]\n",
    "for line in testfile:\n",
    "    if i%5000==0:\n",
    "        print(i)\n",
    "    i=i+1\n",
    "    line=line.decode('utf8')\n",
    "    line=arrangeTweet(line)\n",
    "    sentences_test.append((line[line.find(',')+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test=scikit_TFIDF(12000,2,sentences,sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=X_train[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x=X_train[170000:]\n",
    "test_y=y[170000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logisticReg = linear_model.LogisticRegression(multi_class='ovr')\n",
    "logisticReg.fit(X_train[0:169999], y[0:169999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predict=logisticReg.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.81      0.82     15621\n",
      "          1       0.80      0.83      0.81     14379\n",
      "\n",
      "avg / total       0.82      0.82      0.82     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_predict,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1,max_features=12)\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?',]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())                             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
